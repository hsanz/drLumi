\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[ansinew]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amssymb,amsmath}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{fancyvrb}
\usepackage{enumerate}
\usepackage{float}
\usepackage{setspace}
\usepackage[a4paper,left=2.1cm,right=2.1cm,top=2.4cm,bottom=2.4cm]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{url}
\usepackage[english]{babel}
\usetikzlibrary{shapes.geometric, arrows}

\newenvironment{changemargin1}{
    \begin{list}{}{
        \setlength{\leftmargin}{+5cm}
        \setlength{\rightmargin}{3.5cm}
        \footnotesize
    }
    \item[]
}{\end{list}}

\newenvironment{itemize*}%
    {\begin{itemize}%
        \setlength{\itemsep}{-0.35cm}%
        \setlength{\parskip}{10pt}}%
{\end{itemize}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{drLumi}

%%%%%%%%%%%%%%%%
%%%% First page
%%%%%%%%%%%%%%%%

\title{\Large \bf {\tt drLumi}: multiplex immunoassays data 
analysis \ \vspace{0.5cm}}

\author{ H\'ector Sanz$^{1}$, John Aponte$^{1}$, Jaroslaw Harezlak$^{2}$, \\ 
Yan Dong$^{2}$, Magdalena Murawska$^{2}$ and Clarissa Valim$^{3}$}


\maketitle

\begin{scriptsize}
\noindent $^{1}$ISGlobal, Barcelona Ctr. Int. Health Res. (CRESIB), 
Hospital Cl\'inic - Universitat de Barcelona, Barcelona, Spain\newline
$^{2}$Indiana University Fairbanks School of Public Health, Indianapolis, IN, 
USA \newline
$^{3}$Harvard School of Public Health, Boston, MA, USA
\end{scriptsize} 


\begin{center}
\texttt{hector.sanz@isglobal.org}
\end{center}

\vspace{0.5cm}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%
%%%% Introduction
%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Introduction} 

The {\tt drLumi} package allows users to manage data, calibrate assays and 
perform quality control of multiplex immunoassays. This document provides an
overview on the usage of the main functions of the package and examples
of the implemented methods. The datasets available in the package were used 
in the examples presented so they can be reproduced.\\


\noindent Multiplex immunoassays are used to measure concentrations of several cytokines and chemokines, antibodies, or other proteins simultaneously and are important for biomarkers discovery.\\

\noindent Several beads with antibodies fixed to capture the analyte of interest are mixed with subject's serum or plasma in 96 well plates, one subject per well. Sets of beads are individually labeled according to the specific analyte capability of measuring. After the beads capture the existing analyte in subject's sample, they are allowed to interact with a fluorescent antibody. In the presence of the analyte, the fluorescent antibody and the antibody fixed to the bead make a sandwich with the analyte to be quantified. This complex goes through laser bins that quantify the amount of analyte in each bead with the analyte-antibodies complex; the assay final raw output is the median fluorescence intensity (MFI) over all beads containing the corresponding analytes. To calibrate between-plate variability and quantify analyte concentrations, each plate includes wells containing increasing dilutions of a sample with known concentration of each analyte. Those wells are used to fit standard curves with the MFI as a function of the concentration of the analyte in the reference samples. To calibrate each subject's response, the MFI of the subject samples is converted in analyte concentration using the standard curve. The minimum and maximum concentration of analyte that can be reliably quantified after using the standard curve to transform the MFI in an analyte concentration is called lower and higher limit of quantification (LLOQ and HLOQ), respectively. Additionally, to estimate the background noise in the assay, samples containing no analyte (negative or blank controls) are included in each assay plate.\\

\noindent Several analytical factors when pre-processing the data from the assay may result in suboptimal calibration and decrease assay sensitivity, i.e., failure to quantify a large amount of samples because concentration is outside the limits of quantification (LOQ) range, including the fit of standard curve and the approach used to estimate limits of quantification. Moreover, the approach used to account for background noise when estimating standard curves can also affect the fitting of the standard curves and impact the assay calibration and sensitivity. \\


\noindent There are several commercial software to handle 
concentration-response multiplex data. Usually these software 
allow to estimate concentration of samples using a standard curve but 
they don't allow to perform a good Quality Control (QC) based on residuals, 
goodness of fit or confidence intervals for the coefficients. \\

\newpage
\noindent In the following sections we illustrate how to perform all QC process 
for multiplex immunoassays data using {\tt drLumi} package.

\begin{figure}[h!]
\begin{center}
    \includegraphics[scale=0.8]{./figure/flow.pdf}
\end{center}
\caption{Schematic representation of the main functions and arguments included in the drLumi package. The name of the functions are shown inside blue boxes, options and name of arguments of the functions are shown inside red ellipses and the green box shows the starting point of the flow depending on the origin of the raw data.}
\end{figure}


\noindent The package allows to import CSV raw data that has been 
exported from xPONENT$^\circledR$ software. For a detailed example showing how to 
process this type of data go to section~\ref{sec:xponent}.\\

\noindent To load the package: 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(drLumi)
\end{verbatim}
\end{kframe}
\end{knitrout}



%%%%%%%%%%%%%%%%%%%%
%%%% Type of Data 
%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Main structure of datasets}

The package only allows to read automatically CSV files from xPONENT$^\circledR$ software. 
Other data must be pre-processed, if necessary, and transformed into 
a {\tt data.frame} class object. The final {\tt data.frame} must have the 
following variables:

\begin{itemize*}
    \item Plate identification.
    \item Well position.
    \item Analyte name.
    \item Sample identification.
    \item Median fluorescence intensity values.
    \item Expected concentration.
\end{itemize*}

\noindent The information can be just in one {\tt data.frame} or 
in several (e.g., {\tt mfidata} and {\tt ecdata}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Example data in the package
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Datasets in the package} 

There are three datasets available in the {\tt drLumi} package. 

\subsection{MFI data}
A dataset with the median fluorescence intensity values for three 
96-wells plates and 30 analytes information per plate.  The variables 
included in the dataset are:

\begin{itemize*}
    \item plate: plate identification.
    \item well: position of the sample in the plate.
    \item analyte: analyte tested.
    \item sample: type of sample in the well (blank, standard, positive 
    control or unknown). In each plate 2 blanks,  17 dilution points (standard 
    1 is duplicated), 3 controls (each one duplicated) were tested.
    \item mfi: Median Fluorescence Intensity.
\end{itemize*}

\noindent To load the median fluorescence data:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> data(mfidata)
\end{verbatim}
\end{kframe}
\end{knitrout}


\noindent First 6 rows of the {\tt mfidata}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(mfidata)
    plate  well analyte    sample  mfi
1 plate_1 P1_B1     FGF  Control1 2902
2 plate_1 P1_A2     FGF Standard1 4096
3 plate_1 P1_B2     FGF  Control2  440
4 plate_1 P1_A3     FGF Standard2 3925
5 plate_1 P1_B3     FGF  Control3   40
6 plate_1 P1_A4     FGF Standard4 2510
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{EC data}
A dataset with the expected concentration data by analyte for the 
{\tt mfidata}. The variables included in the {\tt data.frame} are:

\begin{itemize*}
    \item sample: type of sample (background, control or standard).
    \item analyte: analyte tested.
    \item ec: expected concentration value.
\end{itemize*}

\noindent To load the expected concentration data:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> data(ecdata)
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent First 6 rows of the {\tt ecdata}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(ecdata)
     sample analyte        ec
1 Standard1     FGF 3450.0000
2 Standard2     FGF 1725.0000
3 Standard3     FGF  862.5000
4 Standard4     FGF  431.2500
5 Standard5     FGF  215.6250
6 Standard6     FGF  107.8125
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Raw data from xPONENT$^\circledR$ software}
There is an example of CSV raw data from xPONENT$^\circledR$ software in 
{\tt inst/extdata} named \textbf{{\tt plate1.csv}}. For a detailed example
of this data go to section \ref{sec:xponent}.


%%%%%%%%%%%%%%%%%%%
%%%%% Prepare data
%%%%%%%%%%%%%%%%%%%

\section{Extraction of samples data} \label{Extraction of data}

Generally, the machine that reads the assay is connected to a software that produces a dataset including standard points, blank controls and subject's samples. The {\tt data\_selection} function allows splitting variables and samples in different datasets.  Also, is possible to merge the expected concentration values and flag points (samples to be removed from the analysis, for instance outliers). Two available methods can be used in order to identify the samples:

\begin{itemize}
 \item specify the \textit{pattern} of the samples' name. The {\tt data\_selection} 
 function  using regular expressions will match samples based 
 on the \textit{pattern}.
 \item specify the exact name of the samples.
\end{itemize}


\noindent Extracting samples based on a \textit{pattern} and merging the 
expected concentration variables:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> datasets <- data_selection(x = mfidata, ecfile = ecdata, 
+     byvar.ecfile = c("sample","analyte"),
+     backname = "Background0", 
+     stanname="Standard",posname = "Controls")
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent Where

\begin{itemize*}
    \item {\tt x=mfidata}: is the MFI {\tt data.frame}.
\item {\tt ecfile=ecdata}: is the expected concentration {\tt data.frame} to be merged 
to {\tt mfidata}.
\item {\tt byvar.ecfile = c("sample","analyte")}: are the link variables between
the {\tt mfidata} and {\tt ecdata}.
\item {\tt backname = "Background0"}: is the pattern of the blank controls.
\item {\tt stanname = "Standard"}: is the pattern of the standard points.
\item {\tt posname = "Controls"}: is the pattern of the controls other than blank.
\end{itemize*}

\noindent All samples datasets have been extracted 
(first 3 observations of each):\\

\noindent \textbf{Background}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(datasets$plate_1$background,3)
       sample analyte   plate   well  mfi ec
1 Background0     FGF plate_1  P1_A1 21.0  0
2 Background0     FGF plate_1 P1_H12 19.0  0
3 Background0    IL1B plate_1  P1_A1 20.5  0
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent \textbf{Standard}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(datasets$plate_1$standard,3)
     sample analyte   plate  well  mfi   ec
1 Standard1     FGF plate_1 P1_H4 3933 3450
2 Standard1     FGF plate_1 P1_A2 4096 3450
3 Standard2     FGF plate_1 P1_A3 3925 1725
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent \textbf{Positive controls}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(datasets$plate_1$positive,3)
    sample analyte   plate   well    mfi      ec
1 Control1     FGF plate_1  P1_B1 2902.0 1150.00
2 Control1     FGF plate_1 P1_G10 3173.5 1150.00
3 Control2     FGF plate_1  P1_B2  440.0  143.75
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent \textbf{Unknowns}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(datasets$plate_1$unknowns,3)
         sample analyte   plate   well mfi ec
1  M_sid_8_DMSO    IL10 plate_1 P1_F10 521 NA
2  B_sid_13_CSP     FGF plate_1  P1_C1  18 NA
3 B_sid_13_DMSO     FGF plate_1  P1_D1  19 NA
\end{verbatim}
\end{kframe}
\end{knitrout}



%%%%%%%%%%%%%%%%%%%%%%
%%%% Analysis of data
%%%%%%%%%%%%%%%%%%%%%%

\section{Data analysis}
The function used to analyze the data is {\tt scluminex}. Given standard and 
background (optional) datasets, a list of nonlinear models and a background method this 
function tries to fit the list of models hierarchicaly. The package has some 
pre-specified models. The models are fitted by the {\tt nlsLM} function 
from the {\tt minpack.lm} \cite{minpack.lm} package which is a modified 
version of the {\tt nls} function that incorporates the Levenberg-Marquardt 
algorithm. The {\tt scluminex} function transforms the original data into base 
10 logarithm. 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> allanalytes <- scluminex(plateid = "newplate", 
+     standard = datasets$plate_1$standard, 
+     background = datasets$plate_1$background,
+     bkg = "ignore", lfct = c("SSl5","SSl4"), 
+     fmfi = "mfi", verbose = FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent where,

\begin{itemize*}
    \item {\tt plateid}: is the name of the plate (or experiment).
    \item {\tt standard = datasets\$plate\_1\$standard}: is a {\tt data.frame} 
    with the standard points information.
    \item {\tt background = datasets\$plate\_1\$background}: is a {\tt data.frame} 
    with the blank controls data.
    \item {\tt bkg = "ignore"}: is the approach to account for the background noise.
    \item {\tt lfct = c("SSl5","SSl4")}: are the models to be fitted. The function 
    will try to estimate in first place the {\tt SSl5} model and if the model 
    does not converge will try to fit {\tt SSl4}.
    \item {\tt fmfi = "mfi"}: is the name of the MFI variable.
    \item {\tt verbose = FALSE}: logical to do not print the convergence of the models.
\end{itemize*}

\noindent Note that the class of {\tt allanalytes} is {\tt scluminex}. The 
{\tt list} syntax can be used to extract the information of one specific 
analyte:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> class(allanalytes)
[1] "scluminex"
> allanalytes
 [1] "FGF"     "IL1B"    "G-CSF"   "IL10"    "IL13"    "IL6"     "IL12"   
 [8] "RANTES"  "EOTAXIN" "IL17"    "MIP1A"   "GMCSF"   "MIP1B"   "MCP1"   
[15] "IL15"    "EGF"     "IL5"     "HGF"     "VEGF"    "IFNg"    "IFNa"   
[22] "IL1RA"   "TNFa"    "IL2"     "IL7"     "IP10"    "IL2R"    "MIG"    
[29] "IL4"     "IL8"    
> names(allanalytes$FGF)
 [1] "convergence"  "data"         "model"        "coefficients" "rsquare"     
 [6] "aic"          "modelfit"     "neill.method" "flag_data"    "fct"         
[11] "bkg_mean"     "alertbkg"     "bkg_method"   "fieldnames"  
\end{verbatim}
\end{kframe}
\end{knitrout}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Define the models
%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Models}
As described previously, the package has implemented some nonlinear 
{\tt selfStart} models:

\paragraph{Five-parameter logistic function:} using a base 10 
logarithm is implemented though the {\tt SSl5} function:

\begin{align*}
f(x; b, c, d, e, f) = c +\frac{d-c}{(1+10^{b(x-e)})^{f}}
\end{align*}

where,

\begin{itemize*}
    \item $b$: the slope around $e$ parameter
    \item $c$: the lower asymptote parameter
    \item $d$: the upper asymptote parameter
    \item $e$: the concentration that produces a response halfway between 
$c$ and $d$.
    \item $f$: the asymmetry for the slope
\end{itemize*}

\noindent The constraint version has $c$ as a fixed value instead a 
parameter. 

\paragraph{Four-parameter logistic function:} is expressed as a particular version 
of the five-parameter logistic model when the $f$ parameter is fixed to 1:

\begin{align*}
f(x; b, c, d, e) = c +\frac{d-c}{1+10^{b(x-e)}}
\end{align*}

\noindent  The {\tt SSl4} function has the {\tt selfStart} model implemented 
also on base 10 logarithm. As the {\tt SSl5}, the constraint method has the 
parameter $c$ as a fixed value.

\paragraph{Exponential growth:} the {\tt SSexp} function has the 
{\tt selfStart} exponential model implemented on base 10 logarithm:

\begin{align*} 
f(x; y_0, b) = y_0 10^{\frac{x}{b}}
\end{align*}

where,

\begin{itemize*}
    \item $b$: the growth rate
    \item $y_0$: the response when there is no concentration.
\end{itemize*}


\noindent The constraint version of this function has $y_0$ as a fixed value 
instead a parameter. 






\begin{figure}[!ht]
\begin{center}
    \begin{subfigure}{.28\textwidth}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{./unnamed-chunk-15-1} 

\end{knitrout}
    \end{subfigure}
    \begin{subfigure}{.28\textwidth}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{./unnamed-chunk-16-1} 

\end{knitrout}
    \end{subfigure}
    \begin{subfigure}{.28\textwidth}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{./unnamed-chunk-17-1} 

\end{knitrout}
    \end{subfigure}
    \caption{Comparison of five-parameter, four-parameter and exponential 
    models for plate 1 (IL15 analyte) with ignored background.}
\label{fig:modelscomp}
\end{center}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%
%%% Background
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Blank controls}
The {\tt scluminex} functions allows to specify 4 methods to handle 
blank controls. As an example, first we subset information from one 
analyte and the we add the expected concentration data:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> fgf <- subset(mfidata, analyte=="FGF" & plate=="plate_1")
> dat <- data_selection(fgf, ecdata) 
\end{verbatim}
\end{kframe}
\end{knitrout}

\paragraph{Ignored:} the background 
information can be ignored, so the estimation of the coefficients will 
not take into account any background data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ig <- scluminex("plate_1",dat$plate_1$standard, dat$plate_1$background, 
+                 lfct="SSl4", bkg="ignore", fmfi="mfi", fanalyte="analyte", 
+                 verbose=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}


\paragraph{Included:} the estimation of the standard curve takes 
into account the mean of the background of the values as another 
point of the standard curve. The median fluorescence 
intensity and the expected concentration for this new point by  
analyte is estimated as follows:

\begin{itemize*}
    \item MFI: geometric mean value of the blank controls.
    \item EC: the minimum expected concentration value of the standard points 
divided by 2.
\end{itemize*}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> inc <- scluminex("plate_1",dat$plate_1$standard, dat$plate_1$background, 
+                 lfct="SSl4", bkg="include", fmfi="mfi", fanalyte="analyte", 
+                 verbose=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}


\paragraph{Subtracted:} the geometric mean of the blank controls is subtracted 
from all the standard points.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> sub <- scluminex("plate_1",dat$plate_1$standard, dat$plate_1$background, 
+                 lfct="SSl4", bkg="subtract", fmfi="mfi", fanalyte="analyte", 
+                 verbose=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}


\paragraph{Constrained:} the constrained parameter 
(lower asymptote) is fixed to the geometric mean background value and 
$p-1$ parameters are estimated from the original model. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> cons <- scluminex("plate_1", dat$plate_1$standard, dat$plate_1$background, 
+     lfct="SSl4", bkg="constraint", fmfi="mfi", fanalyte="analyte", 
+     verbose=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}



\noindent Figure \ref{fig:backcomp} shows the comparison of the 4 methods.




\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.6]{./figure/comparisonbackground.pdf}
\caption{Comparison of background methods for plate 1 (FGF analyte) 
and 4 parameters logistic model. Green line shows the geometric mean of the
blank controls.}
\label{fig:backcomp}
\end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Limits of Quantification
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage

\subsection{Limits of quantification}
The package implements 3 types of estimation for the upper and lower 
limit of quantification.

\paragraph{Derivatives:} the estimation is based on the second order 
derivative of the model \cite{loqder}. Once is estimated the maximum and 
minimum values are found (finding the roots on the third derivative) and 
those are the limits of quantification (Figure \ref{fig:loqder}). The package 
calculates the exact derivatives functions for the {\tt SSl5}, 
{\tt SSl4} and {\tt SSexp}. Given an {\tt scluminex} object the 
{\tt loq\_derivatives} function estimates the LOQ for all analytes 
or the specified ones by the {\tt subset.list} argument.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # arguments of the function
> args(loq_derivatives)
function (x, subset.list = NULL, ...) 
NULL
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> der <- loq_derivatives(allanalytes, subset.list="FGF")
> der
  analyte     lloq     uloq     method
1     FGF 1.088223 2.250466 derivative
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{./figure/loqder.pdf}
\caption{Representation of the limits of quantification based on derivatives.}
\label{fig:loqder}
\end{center}
\end{figure}



\clearpage
\paragraph{Interval:} the {\tt loq\_interval} function estimates the 
LOQ based on the prediction interval of the curve and the coefficients 
of the model \cite{loqint}:

\begin{itemize*}
\item Lower limit of quantification: is the concentration 
value of the intersection between lower prediction interval of the 
standard curve and the upper interval for the lower asymptote coefficient 
estimated by the model (if the model allows).
\item Upper limit of quantification: is the concentration 
value of the intersection between upper prediction interval of the standard 
curve and the lower interval for the upper asymptote coefficient 
estimated by the model (if the model allows).
\end{itemize*}

\noindent The function needs as an argument, the model and the position 
or name of the asymptote coefficients. In Figure \ref{fig:loqint} there is an 
example for the plate 1, FGF analyte with ignored background.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> int <- loq_interval(allanalytes, subset.list= "FGF", low.asymp=2, high.asymp=3)
> int
  analyte     lloq     uloq   method
1     FGF 0.677414 2.985435 interval
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{./loqint-1} 

}

\caption[Estimation of limits of quantification based on interval method for plate 1, FGF analyte and ignored background]{Estimation of limits of quantification based on interval method for plate 1, FGF analyte and ignored background}\label{fig:loqint}
\end{figure}


\end{knitrout}



\noindent In the scenario where one of the asymptotes must be specified 
and not based in the coefficients, the arguments {\tt lowci} or {\tt highci} 
must be changed into the desired value. For example, in Figure 
\ref{fig:loqint2} the upper LOQ is estimated based on coefficients 
(same values as estimated in Figure \ref{fig:loqint}) but the lower 
asymptote has been fixed to 1.5 (not based on the 
coefficients estimation). Therefore, the intersection of the lower asymptote 
with the prediction interval is different and consequently the 
estimation of the concentration:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> int2 <- loq_interval(allanalytes, subset.list="FGF", high.asymp=3, lowci=1.5)
> int2
  analyte      lloq     uloq   method
1     FGF 0.9690558 2.985435 interval
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{./loqint2-1} 

}

\caption[Estimation of limits of quantification based on interval method (fixed lower assymptote value) for plate 1, FGF analyte and ignored background]{Estimation of limits of quantification based on interval method (fixed lower assymptote value) for plate 1, FGF analyte and ignored background.}\label{fig:loqint2}
\end{figure}


\end{knitrout}



\paragraph{Coefficient of variation:} this method is based on the estimation of 
the coefficient of variation of the fitted concentration values 
(base 10 logarithm) \cite{Defawe,Gottschalk}. The function calls the {\tt invest} 
function and estimates the fitted concentration given a MFI, the standard error 
is estimated using the Delta Method. The Coefficient of Variation for the 
fitted concentration is estimated as:

\begin{align*}
\sqrt{e^{ (SE \times  ln(10))^2} - 1 }
\end{align*}

\noindent where $SE$ is the standard error of the fitted 
concentration \cite{logcv}. \\

\noindent For a specific coefficient of variation cutoff the LLOQ and HLOQ are calculated as the fitted concentration values whose coefficient of variation is lower or equal to the specified cutoff. \\

\noindent Given a {\tt scluminex} object and a coefficient of variation cutoff 
({\tt max.cv} argument), the {\tt loq\_cv} function estimates the LOQ. See
Figure \ref{fig:loqcv} for a representation of it.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> cv <- loq_cv(allanalytes, subset.list="FGF", max.cv=0.2)
> cv
  analyte      lloq     uloq                      method
1     FGF 0.4623362 3.127466 coefficient variation <=0.2
\end{verbatim}
\end{kframe}
\end{knitrout}




\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{./loqcv-1} 

}

\caption[Estimation of limits of quantification based on coefficient of variation method]{Estimation of limits of quantification based on coefficient of variation method.}\label{fig:loqcv}
\end{figure}


\end{knitrout}


\noindent The LOQ object is {\tt loq} class:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> class(der)
[1] "loqderivatives" "loq"           
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent the {\tt summary} method can be applied to obtain more information.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> summary(der)
  analyte     lloq     uloq       ly       uy loq.drange y.drange     method
1     FGF 1.088223 2.250466 1.681452 3.097439   1.162243 1.415988 derivative
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The new variables added are:
\begin{itemize*}
    \item {\tt ly}: the MFI value of the LLOQ ({\tt lloq} variable).
    \item {\tt uy}: the MFI value of the ULOQ ({\tt uloq} variable).
    \item {\tt loq.drange}: dynamic range of the LOQ (difference between {\tt uloq}
    and {\tt lloq}).
    \item {\tt y.drange}: dynamic range of the MFI values (difference between {\tt uy}
    and {\tt ly}).
\end{itemize*}

\subsection{Fitted concentration}
Given a MFI value (base 10 logarithm), the function {\tt invest} 
estimates the concentration value and the standard error 
calculating the invert value \cite{INLR}. The package calculates 
the inverse functions for the 5-parameters, 4-parameters and exponential 
functions (in base 10 logarithm) therefore an analytically solution 
is given. The arguments of the function are:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> args(invest)
function (x, analyte = NULL, yvalue, ci.method = c("delta", "bootstrap"), 
    level = 0.95, seed.boot = 123, nboot = 100) 
NULL
\end{verbatim}
\end{kframe}
\end{knitrout}


\noindent Two methods for estimating the confidence interval are available:

\paragraph{Bootstrap:} generates {\tt nboot} response vectors 
(assuming normality) and fit the same inital model with the original 
concentration data. The confidence interval is calculated by the 
percentile method specified in the {\tt level} argument. No standard error 
is estimated.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> invesboot <- invest(ig, "FGF", yvalue = 1.4, ci.method="bootstrap")
> invesboot
  log10_mfi log10_concentration.fit log10_concentration.lci
1       1.4               0.4949141               0.3766864
  log10_concentration.uci log10_concentration.se ci.method analyte
1               0.5770771                     NA bootstrap     FGF
\end{verbatim}
\end{kframe}
\end{knitrout}


\paragraph{Delta method:} estimates the standard error based on 
the Delta Method. The function used is {\tt deltamethod}  from the 
{\tt msm} \cite{msm} package.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> invesdelta <- invest(ig, "FGF", yvalue = 1.4, ci.method="delta")
> invesdelta
  log10_mfi log10_concentration.fit log10_concentration.lci
1       1.4               0.4949141                0.383485
  log10_concentration.uci log10_concentration.se ci.method analyte
1               0.6063431             0.05157872     delta     FGF
\end{verbatim}
\end{kframe}
\end{knitrout}



\noindent Also is possible to take into account the dilution of 
the concentration calling the {\tt est\_conc} function. 
This function is a wrapper of the {\tt invest} one but is specific for 
concentration estimation. Given a {\tt scluminex} object and a dataset, 
the function estimates the concentration for each analyte with the 
corresponding estimated model. \\


\noindent As an example, to select the positive control dataset of the FGF analyte:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> concdf <- subset(datasets$plate_1$positive, analyte=="FGF")
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent Assuming dilution factor 1 (same results as {\tt invest} function):
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> est_conc(ig, concdf, fmfi="mfi", dilution=1)
    sample analyte   plate   well    mfi        ec warning log10.fitted.conc
1 Control1     FGF plate_1  P1_B1 2902.0 1150.0000                 2.7670472
2 Control1     FGF plate_1 P1_G10 3173.5 1150.0000                 2.8704865
3 Control2     FGF plate_1  P1_B2  440.0  143.7500                 1.8702911
4 Control2     FGF plate_1 P1_G11  435.0  143.7500                 1.8667074
5 Control3     FGF plate_1  P1_B3   40.0   21.2963                 0.9635245
6 Control3     FGF plate_1 P1_G12   36.0   21.2963                 0.8886454
  log10.fitted.conc.se dilution dil.fitted.conc dil.lb.conc dil.ub.conc
1           0.03614467        1      584.853627  488.608904  700.056348
2           0.04404870        1      742.141174  596.108199  923.948911
3           0.01475733        1       74.180727   68.930244   79.831144
4           0.01472403        1       73.571118   68.375109   79.161987
5           0.02747050        1        9.194423    8.020068   10.540734
6           0.02966791        1        7.738296    6.676544    8.968896
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent Assuming dilution factor 2:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> est_conc(ig, concdf, fmfi="mfi", dilution=2)
    sample analyte   plate   well    mfi        ec warning log10.fitted.conc
1 Control1     FGF plate_1  P1_B1 2902.0 1150.0000                 2.7670472
2 Control1     FGF plate_1 P1_G10 3173.5 1150.0000                 2.8704865
3 Control2     FGF plate_1  P1_B2  440.0  143.7500                 1.8702911
4 Control2     FGF plate_1 P1_G11  435.0  143.7500                 1.8667074
5 Control3     FGF plate_1  P1_B3   40.0   21.2963                 0.9635245
6 Control3     FGF plate_1 P1_G12   36.0   21.2963                 0.8886454
  log10.fitted.conc.se dilution dil.fitted.conc dil.lb.conc dil.ub.conc
1           0.03614467        2      1169.70725   977.21781  1400.11270
2           0.04404870        2      1484.28235  1192.21640  1847.89782
3           0.01475733        2       148.36145   137.86049   159.66229
4           0.01472403        2       147.14224   136.75022   158.32397
5           0.02747050        2        18.38885    16.04014    21.08147
6           0.02966791        2        15.47659    13.35309    17.93779
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Agreement between controls}

The function {\tt intra\_icc} estimates the intraclass 
correlation coefficient for a dataset in long format. The function calls the
{\tt icc} function from the {\tt irr} \cite{irr} package.\\

\noindent Taking as example the concentration data for the positive controls:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> conc_icc_df <- est_conc(allanalytes, datasets$plate_1$positive, 
+     fmfi="mfi", dilution=1)
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The data has the following structure (6 first rows):
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(conc_icc_df)
    sample analyte   plate   well    mfi        ec warning log10.fitted.conc
1 Control1     FGF plate_1  P1_B1 2902.0 1150.0000                 2.8017672
2 Control1     FGF plate_1 P1_G10 3173.5 1150.0000                 2.9039833
3 Control2     FGF plate_1  P1_B2  440.0  143.7500                 1.8590243
4 Control2     FGF plate_1 P1_G11  435.0  143.7500                 1.8553082
5 Control3     FGF plate_1  P1_B3   40.0   21.2963                 0.9894147
6 Control3     FGF plate_1 P1_G12   36.0   21.2963                 0.9231057
  log10.fitted.conc.se dilution dil.fitted.conc dil.lb.conc dil.ub.conc
1           0.03951173        1      633.529984  519.611216  772.424129
2           0.04442805        1      801.647207  641.479648 1001.806132
3           0.01525222        1       72.281029   66.956476   78.029004
4           0.01525136        1       71.665174   66.386276   77.363839
5           0.02835275        1        9.759211    8.465241   11.250972
6           0.03269666        1        8.377331    7.109937    9.870645
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent To estimate the ICC:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> icc_positive <- intra_icc(conc_icc_df, id.var=c("sample", "analyte", "plate"), 
+     value.var="dil.fitted.conc", type="agreement",model="twoway", 
+     unit="single")
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent where

\begin{itemize*}
    \item {\tt id.var}: indetifies the replicates samples
    \item {\tt value.var}: the variable to be analyzed.
    \item others: arguments to be passed to the {\tt icc} function from the
{\tt irr} package
\end{itemize*}


\noindent There are three objects in the list generated:\\

\noindent \textbf{Re-Structured dataset}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(icc_positive$icc.df)
  dil.fitted.conc_1 dil.fitted.conc_2   sample analyte   plate
1        633.529984        801.647207 Control1     FGF plate_1
2         72.281029         71.665174 Control2     FGF plate_1
3          9.759211          8.377331 Control3     FGF plate_1
4       1209.316230       1272.055684 Control1    IL1B plate_1
5        167.309524        165.716522 Control2    IL1B plate_1
6         19.680236         21.636014 Control3    IL1B plate_1
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent \textbf{The ICC object from the {\tt irr} package} 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> names(icc_positive$icc.mod)
 [1] "subjects"   "raters"     "model"      "type"       "unit"      
 [6] "icc.name"   "value"      "r0"         "Fvalue"     "df1"       
[11] "df2"        "p.value"    "conf.level" "lbound"     "ubound"    
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent \textbf{The ICC estimation}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> icc_positive$icc.value
[1] 0.9871378
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Summary of results}

The {\tt scluminex} object can be printed, summarized and plotted.


\paragraph{Print:} the name of the analytes is listed.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> allanalytes
 [1] "FGF"     "IL1B"    "G-CSF"   "IL10"    "IL13"    "IL6"     "IL12"   
 [8] "RANTES"  "EOTAXIN" "IL17"    "MIP1A"   "GMCSF"   "MIP1B"   "MCP1"   
[15] "IL15"    "EGF"     "IL5"     "HGF"     "VEGF"    "IFNg"    "IFNa"   
[22] "IL1RA"   "TNFa"    "IL2"     "IL7"     "IP10"    "IL2R"    "MIG"    
[29] "IL4"     "IL8"    
\end{verbatim}
\end{kframe}
\end{knitrout}

\paragraph{Summary:} a dataset is generated showing the estimated coefficients, 
number of observations, $R^2$, convergence and the fitted function for each
analyte. To extract more information {\tt as.data.frame} can be applied to a
{\tt summary.scluminex} object or to a {\tt scluminex}. The methodology applied:


\subparagraph{Neill test:} is an ANOVA-based lack-of-fit test \cite{NEILLTEST}.
The method does not require replicates for concentration values but assumes 
that predictor variable can be grouped. The function is an adaptation 
of the {\tt neill.test} from {\tt drc} package \cite{DRC}. The p-value 
of the test is reported.

\subparagraph{$ \boldsymbol{R^2}$:} is the adjusted version of $R^2$ which 
takes into account the number of fitted parameters, estimated as:

\begin{align*}
\left( 1 - \frac{\frac{SS_{err}}{(n-p-1)}}{\frac{SS_{tot}}{(n-1)}} \right)
\end{align*}

where,
\begin{itemize*}
    \item $n$: number of observations
    \item $p$: number of estimated coefficients 
    \item $SS_{err}$: residual sum of squares
    \item $SS_{tot}$: total sum of squares 
\end{itemize*}

\subparagraph{AIC:} Akaike information criterion estimated as:

\begin{align*}
\left(  -2 \times (log-likelihood) + 2 \times p \right)
\end{align*}

\noindent where $p$ is the number of estimated parameters in the model. The 
function applied to the fitted model is the generic {\tt AIC} function for 
{\tt nls} class object. \\

% note that for {\tt nls} models the number of estimated parameters
% $p$ is (number of coefficients + 1)
% so AIC(nls_model) = -2*logLik(model) + (2*(length(coef(model))+1))

\noindent As example, the {\tt summary} method applied to a {\tt scluminex}
object




\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> summary(allanalytes)
   analyte          b          c        d         e         f obs   rsquare convergence  fct
1      FGF -0.8828771  1.3068483 3.683054 1.3890770 1.7678441  17 0.9986956 convergence SSl5
2     IL1B -0.7291501  1.2823197 4.157377 2.2406824 1.1845345  17 0.9997494 convergence SSl5
3    G-CSF -0.7229726  0.7586540 4.092740 2.6835136 1.2948285  17 0.9994358 convergence SSl5
4     IL10 -0.4523784  0.6016439 4.595441 1.9790133 0.9125607  17 0.9994614 convergence SSl5
5     IL13 -0.8043012  0.8938679 4.283454 1.9005612 1.6165574  17 0.9996548 convergence SSl5
6      IL6 -0.6209108  0.9814027 4.348758 1.9220673 0.7637184  17 0.9992782 convergence SSl5
7     IL12 -0.6862771  0.8229139 4.204463 2.2984784 1.3830266  17 0.9998644 convergence SSl5
8   RANTES -0.9437332  1.7723069 3.918469 1.6347323 1.7387868  17 0.9994649 convergence SSl5
9  EOTAXIN -0.5210368  1.4316138 4.289606 0.8253446 2.1946897  17 0.9997680 convergence SSl5
10    IL17 -0.7770365  0.8729686 4.437082 2.6606798 0.5119282  17 0.9996918 convergence SSl5
11   MIP1A -0.7290696  0.9101793 3.990784 1.5526541 1.9647135  17 0.9998835 convergence SSl5
12   GMCSF -0.4091215  0.3682323 4.531576 1.2278351        NA  17 0.9991905 convergence SSl4
13   MIP1B -0.5502088  1.0419523 4.381991 2.0851423 1.5433827  17 0.9997014 convergence SSl5
14    MCP1 -0.6368871  1.0210596 4.558936 2.2773092 1.4159834  17 0.9998101 convergence SSl5
15    IL15 -1.2741555  1.3014988 3.801743 3.2553293 0.4213175  17 0.9989005 convergence SSl5
16     EGF -0.7775902  1.4265917 4.247292 1.9743882 0.7942679  17 0.9999056 convergence SSl5
17     IL5 -0.7358589 -0.1865238 4.482711 2.5280240 0.2742499  17 0.9994644 convergence SSl5
18     HGF -0.4779733  0.8688056 5.138004 2.5832461 2.0644387  17 0.9996647 convergence SSl5
19    VEGF -0.8248516  1.5726924 3.950033 1.6801506 1.0039745  17 0.9997100 convergence SSl5
20    IFNg -0.4707418  0.6146812 4.754997 2.3781928        NA  17 0.9992237 convergence SSl4
21    IFNa -0.9684408  0.7668357 4.234176 2.2819768 0.9519239  17 0.9993487 convergence SSl5
22   IL1RA -0.6535843  1.6186756 3.973113 3.1939268        NA  17 0.9977053 convergence SSl4
23    TNFa -0.3705635  0.7370135 4.773413 0.9077849 2.1495834  17 0.9994428 convergence SSl5
24     IL2 -0.6672199  0.9780458 4.410199 2.3856437 0.7218382  17 0.9998813 convergence SSl5
25     IL7 -0.8784672  1.5961995 4.267243 2.4553340 1.1748884  17 0.9997071 convergence SSl5
26    IP10 -0.7080650  0.7865411 4.353835 0.9641264 1.5094283  17 0.9996376 convergence SSl5
27    IL2R -0.5859247  0.9110713 4.285251 2.7511816 1.2448516  17 0.9996893 convergence SSl5
28     MIG -0.6499398  0.8284273 4.598374 1.9951056 1.8461333  17 0.9996634 convergence SSl5
29     IL4 -0.5490025  0.8743643 4.339536 2.1432739 1.3927581  17 0.9998933 convergence SSl5
30     IL8 -0.6469705  1.7275331 4.135944 2.0599591 1.4727478  17 0.9996691 convergence SSl5
\end{verbatim}
\end{kframe}
\end{knitrout}




\noindent the {\tt as.data.frame} method to a {\tt scluminex} object
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> as.data.frame(ig)
   analyte    mfi           ec   well log10_mfi log10_concentration
1      FGF 4096.0 3450.0000000  P1_A2  3.612360          3.53781910
2      FGF 3933.0 3450.0000000  P1_H4  3.594724          3.53781910
3      FGF 3925.0 1725.0000000  P1_A3  3.593840          3.23678910
4      FGF 2510.0  431.2500000  P1_A4  3.399674          2.63472911
5      FGF  675.0  107.8125000  P1_A5  2.829304          2.03266912
6      FGF 3854.0  862.5000000  P1_H5  3.585912          2.93575910
7      FGF  123.5   26.9531250  P1_A6  2.091667          1.43060913
8      FGF 1377.0  215.6250000  P1_H6  3.138934          2.33369911
9      FGF   30.0    6.7382812  P1_A7  1.477121          0.82854913
10     FGF  303.0   53.9062500  P1_H7  2.481443          1.73163912
11     FGF   22.0    1.6845703  P1_A8  1.342423          0.22648914
12     FGF   51.0   13.4765625  P1_H8  1.707570          1.12957913
13     FGF   20.0    0.4211426  P1_A9  1.301030         -0.37557085
14     FGF   25.0    3.3691406  P1_H9  1.397940          0.52751914
15     FGF   22.0    0.1052856 P1_A10  1.342423         -0.97763084
16     FGF   21.0    0.8422852 P1_H10  1.322219         -0.07454085
17     FGF   19.0    0.2105713 P1_H11  1.278754         -0.67660084
                       warning predicted.log10_mfi    residuals
1                                         3.619779 -0.204908310
2                                         3.619779 -0.691997531
3                                         3.586877  0.192313387
4                                         3.399745 -0.001967785
5                                         2.859326 -0.829174292
6                                         3.522083  1.762897543
7                                         2.042835  1.348683934
8                                         3.186101 -1.302704181
9                                         1.523880 -1.291419811
10                                        2.448572  0.907867003
11                                        1.347986 -0.153657570
12                                        1.727064 -0.538396036
13                                        1.302009 -0.027032456
14                                        1.408672 -0.296396611
15                                        1.290855  1.424237036
16                                        1.317258  0.137038697
17 Not_Estimated_Concentration            1.294517 -0.435383042
   log10.fitted.conc log10.fitted.conc.se plateid
1        3.449503479           0.20302133 plate_1
2        3.292007536           0.13140535 plate_1
3        3.285427496           0.12902475 plate_1
4        2.634596941           0.02984423 plate_1
5        2.009076031           0.01649710 plate_1
6        3.230453096           0.11075190 plate_1
7        1.469657477           0.01615612 plate_1
8        2.282880986           0.02131608 plate_1
9        0.727426969           0.03567570 plate_1
10       1.754876098           0.01406998 plate_1
11       0.185252694           0.10496266 plate_1
12       1.106420774           0.02375991 plate_1
13      -0.404669644           0.48753881 plate_1
14       0.486818796           0.05239479 plate_1
15       0.185252694           0.10496266 plate_1
16      -0.009466399           0.17418522 plate_1
17               NaN                  NaN plate_1
\end{verbatim}
\end{kframe}
\end{knitrout}



\noindent and {\tt as.data.frame} to a {\tt summary.scluminex} method:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ss <- summary(ig)
> as.data.frame(ss)
  analyte         b        c        d        e       b_se       c_se       d_se
1     FGF -1.039396 1.287375 3.651836 1.746509 0.04389603 0.01758703 0.02389215
        e_se       b_t      c_t      d_t      e_t       b_pval       c_pval
1 0.01906806 -23.67858 73.20025 152.8467 91.59341 4.469256e-12 2.148862e-18
       d_pval       e_pval obs   rsquare modelfit       aic convergence plateid
1 1.51517e-22 1.172038e-19  17 0.9985489    0.299 -59.14575 convergence plate_1
   fct bkg_mean bkg_method
1 SSl4 19.97498     ignore
\end{verbatim}
\end{kframe}
\end{knitrout}

\newpage
\paragraph{Plot:} standard curves, standardized residuals or 
Q-Q plot of the residuals are plotted. The function is based on 
{\tt ggplot2} so other data can be added to the plot. The plot 
is specified by the {\tt type} argument.\\

\noindent The default type is {\tt scurve} and the function 
allows to plot the standard curve of a plate for all analytes 
or just the ones desired. Also allows to plot some other aspects 
as legend, confidence bands, background or specify the number of columns. 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(allanalytes, type = "scurve", ncol=5, psize=1)
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{./unnamed-chunk-49-1} 

}



\end{knitrout}

\newpage
\noindent The type {\tt residuals} show the standardized residuals. 
The points beyond the {\tt out.limit} argument are presented in red and 
the well variable is shown.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(allanalytes, type = "residuals", out.limit= 2.5, ncol=5, psize=1)
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{./unnamed-chunk-50-1} 

}



\end{knitrout}

\newpage
\noindent The type {\tt qqplot} generates the Q-Q plot of the 
standardized residuals.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(allanalytes,  type = "qqplot", ncol=5, psize=1)
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{./unnamed-chunk-51-1} 

}



\end{knitrout}

\section{Flag data}

\noindent The package implements several features to easily identify outliers.
The function {\tt get\_outliers} allows to identify the points of the standard
curve with a standardized residual greater than a specified value.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> get_outliers(allanalytes, out.limit=2)
        batch_well_analyte    batch   well analyte    flag observations
6       newplate*P1_H5*FGF newplate  P1_H5     FGF OUTLIER     2.153402
30     newplate*P1_A9*IL1B newplate  P1_A9    IL1B OUTLIER    -2.220823
85    newplate*P1_H11*IL13 newplate P1_H11    IL13 OUTLIER    -2.266228
142 newplate*P1_H5*EOTAXIN newplate  P1_H5 EOTAXIN OUTLIER     2.127560
309    newplate*P1_A3*VEGF newplate  P1_A3    VEGF OUTLIER    -2.136298
474    newplate*P1_A10*MIG newplate P1_A10     MIG OUTLIER    -2.313133
482     newplate*P1_H5*IL4 newplate  P1_H5     IL4 OUTLIER     2.099858
509    newplate*P1_H10*IL8 newplate P1_H10     IL8 OUTLIER    -2.236779
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent As commented previously the plot of the residuals also allows to 
identify the same points

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(allanalytes, "residuals", out.limit=2, size.text=2.5)
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{./unnamed-chunk-53-1} 

}



\end{knitrout}

\noindent Once we have identified the outliers we can add this information
to the data, using {\tt data\_selection} or the {\tt merge} function. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> out <- get_outliers(allanalytes, out.limit=2)
> flag.dat <- merge(datasets$plate_1$standard, out, by=c("analyte","well"),all.x=TRUE)
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent After we can run the {\tt scluminex} function.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> flag.allanalytes <- scluminex(plateid = "newplate.flag", 
+     standard = flag.dat, 
+     background = datasets$plate_1$background,
+     bkg = "ignore", lfct = c("SSl5","SSl4"), 
+     fmfi = "mfi", verbose = FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent And plot the standard curve. The flagged points are shown as 
non-filled points but they are not included in the estimation of the curve.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(flag.allanalytes, "scurve", 
+      subset.list=c("FGF","IL1B", "IL13","EOTAXIN","VEGF","MIG","IL4","IL8"),
+      ncol=4, psize=2, size.legend=3)
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{./unnamed-chunk-56-1} 

}



\end{knitrout}




\section{Raw data from xPONENT$^\circledR$ software}
\label{sec:xponent}
The package allows to import CSV raw data that has been exported from 
xPONENT$^\circledR$ software. The {\tt lum\_import} function identifies sections of 
information from this data. Moreover the function imports Bead raw 
data. 

\subsection{MFI raw data}
\noindent There is an example of MFI raw data included in the package. The CSV 
file has several blocks of information that need to be extracted and 
restructured in order to analyze it. This raw data can be imported calling 
the {\tt lum\_import} function:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> imp_path <-  system.file(c("inst","extdata"),"plate1.csv", package="drLumi")
> imp <- lum_import(imp_path)
> imp
The file imported is: Fluorescence type 
Identified data type: Median 
Identified data type: Net MFI 
Identified data type: Count 
Identified data type: Result 
Identified data type: Range 
Identified data type: Avg Net MFI 
Identified data type: Avg Result 
Identified data type: Avg Range 
Identified data type: %CV Replicates 
Identified data type: % Recovery 
Identified data type: Comments 
Identified data type: Units 
Identified data type: Standard Expected Concentration 
Identified data type: Control Expected Concentration 
Identified data type: Control Range - Low 
Identified data type: Control Range - High 
Identified data type: Per Bead Count 
Identified data type: Dilution Factor 
Identified data type: Analysis Types 
Identified data type: Analysis Coefficients 
Identified data type: R^2 
Identified data type: Audit Logs 
Identified data type: Warnings/Errors 
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The function identifies the section parts of the CSV file and groups 
the data in several datasets. The {\tt lum\_import} object has the 
following objects:

\begin{itemize*}
    \item {\tt dtblock}: blocks of information from original CSV file.
    \item {\tt raw\_metadata}: a {\tt data.frame} with the information 
    of batch (software version, operator, batch date  \ldots). 
    \item {\tt vars} type object: variables that are going to be 
    exported in the {\tt lum\_export} function. These variables can 
    be modified in order to remove or add more. 
    \item {\tt name\_batch}: the name of the batch as it is described 
    in raw data.
    \item {\tt type\_raw\_data}: Fluorescence (MFI values for samples) 
    or Bead (for Bead data).
\end{itemize*}

\noindent After the identification of the raw data is necessary to 
extract the information from the {\tt lum\_import} object. This 
can be done using the {\tt lum\_export} function which generates several 
datasets based on the identified sections and the specified variables:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> expdb <- lum_export(imp)
> expdb
Dataframes: 
well scurve average batch region sample name_batch
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent As described previously variables can be removed  
(or new ones can be added). Following is an example for the selection of 2 
variables from the well dataset:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> imp$well_vars
[1] "Median"     "Net MFI"    "Count"      "Result"     "Range"     
[6] "% Recovery" "Comments"  
> imp$well_vars <- c("Median", "Net MFI")
> exp <- lum_export(imp)
> head(exp$well)
  batch_well_analyte   batch  well analyte        sample median net_mfi
1  plate_1*P1_A1*FGF plate_1 P1_A1     FGF   Background0     21       1
2  plate_1*P1_B1*FGF plate_1 P1_B1     FGF      Control1   2902    2882
3  plate_1*P1_C1*FGF plate_1 P1_C1     FGF  B_sid_13_CSP     18      -2
4  plate_1*P1_D1*FGF plate_1 P1_D1     FGF B_sid_13_DMSO     19      -1
5  plate_1*P1_E1*FGF plate_1 P1_E1     FGF  B_sid_13_HBS     17      -3
6  plate_1*P1_F1*FGF plate_1 P1_F1     FGF B_sid_13_AMA1     18      -2
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Bead raw data}
Bead raw data has several files (usually one file per well). This type of data
can be imported either from a folder or from a zip file. The 
function assumes that all well files within the folder are in CSV format and 
try to combine all information. To identify the files new variables are added:

\begin{itemize*}
\item well: with the name of the CSV file
\item batch: the name of the file
\item batch\_well\_eventno: a combination of well, batch and eventno variables.
\end{itemize*}

\noindent Reading non-zip compressed data:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> imp_path_nozip <- system.file(c("inst","extdata"),"bead_data", 
+                               package="drLumi")
> bead_nozip <- lum_import(imp_path_nozip)
> bead_nozip
The file imported is: Bead type 
Number of unique wells files 2 
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent The code for reading zip files is the same as reading
non-compressed files and returns the same information. The only difference is
that first unzip the file and creates a new folder with the same name as the original
in the same directory where the zip file is located.

\noindent All CSV files are combined in one {\tt data.frame}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> head(bead_nozip$bead_files)
  eventno rid dbl    dd rp1 cl1 cl2 aux1 time        well     batch
1       0   0   0 24977  13 117   0    0    0 plate_P1_A1 bead_data
2       1   0   0 25917   0 281  10    0    0 plate_P1_A1 bead_data
3       2   0   0 26555   0 394  17    0    0 plate_P1_A1 bead_data
4       3   0   0 24832   0  68   0    0    0 plate_P1_A1 bead_data
5       4   0   0 28166   0 419  29    0    0 plate_P1_A1 bead_data
6       5   0   0 27034   0 404  29    0    0 plate_P1_A1 bead_data
       batch_well_eventno
1 bead_data*plate_P1_A1*0
2 bead_data*plate_P1_A1*1
3 bead_data*plate_P1_A1*2
4 bead_data*plate_P1_A1*3
5 bead_data*plate_P1_A1*4
6 bead_data*plate_P1_A1*5
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent And the files are identifiable by the well variable:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> with(bead_nozip$bead_files, table(well, batch))
             batch
well          bead_data
  plate_P1_A1      4981
  plate_P1_A2      4647
\end{verbatim}
\end{kframe}
\end{knitrout}


\newpage
%\bibliographystyle{bmc_article}  % Style BST file
\bibliographystyle{acm}  % Style BST file
\bibliography{drLumi_vignette}


\end{document}


